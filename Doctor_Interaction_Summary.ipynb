{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["Aa1yXUesL7Zy","YhyMoqjoq48A","AFBNy7vSq42O","wCmKWuZkBzzR"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UmKLmjqmHMQM"},"source":["<center><p float=\"center\">\n","  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n","  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n","</p></center>\n","\n","<center><font size=10>Generative AI for Business Applications</center></font>\n","<center><font size=6>Fine-Tunning LLMs - Week 1</center></font>"]},{"cell_type":"markdown","metadata":{"id":"azxWym9-HMpz"},"source":["<center><p float=\"center\">\n","  <img src=\"https://images.pexels.com/photos/5699431/pexels-photo-5699431.jpeg\" width=720></a>\n","<center><font size=6>Fine-Tuned AI for Summarizing Medical Conversations</center></font>"]},{"cell_type":"markdown","metadata":{"id":"Aa1yXUesL7Zy"},"source":["# **Problem Statement**"]},{"cell_type":"markdown","metadata":{"id":"zFN8LrodMR7w"},"source":["## **Business Context**"]},{"cell_type":"markdown","source":["In today’s fast-paced healthcare environment, doctors often struggle with the time-consuming task of documenting patient consultations. These conversations are usually long, unstructured, and difficult to review later, which makes writing and editing notes a heavy burden. The inefficiencies carry over into follow-ups, as important details may be buried within lengthy transcripts. To address this challenge, a healthcare technology team is developing an LLM-powered assistant designed to streamline clinical documentation. This AI assistant will automatically generate structured summaries, highlight key patient details, and organize consultation notes for easy review. By applying this solution to real-world consultation data, the team aims to demonstrate how advanced language models can reduce administrative workload, minimize errors, and ultimately improve the quality of patient care."],"metadata":{"id":"RVirgCoA7-yg"}},{"cell_type":"markdown","source":["##  **Objective**"],"metadata":{"id":"qDG_248npouB"}},{"cell_type":"markdown","source":["The goal is to develop an AI-powered system that demonstrates how Natural Language Processing (NLP) can support doctors by automatically generating clear and structured consultation notes from patient interactions.\n","\n","Specifically, the system aims to:\n","\n","* Capture patient concerns, including symptoms, medical history, and lifestyle factors.\n","* Summarize the doctor’s findings and recommendations, such as diagnoses, prescriptions, tests, and follow-up actions.\n","* Ensure that all notes maintain a consistent, professional clinical tone.\n","* Improve accuracy and reliability by fine-tuning on domain-specific medical dialogues.\n","\n","This case study focuses on building a prototype that transforms unstructured doctor-patient conversations into concise, structured clinical summary. By doing so, it reduces the time spent on manual documentation, minimizes the risk of missed information, and enhances the overall quality of patient care.\n","\n","Through successful implementation, the organization seeks to reduce administrative workload, support more efficient follow-ups, and ultimately improve healthcare outcomes.\n"],"metadata":{"id":"_0tk7AzC8SpR"}},{"cell_type":"markdown","source":["## **Data Description**"],"metadata":{"id":"IGxmR0XVwcAi"}},{"cell_type":"markdown","source":["The dataset is divided into three files: training, validation, and test. It consists of two primary columns:\n","\n","1. **Conversation** - Contains the raw transcripts of doctor-patient dialogues, which are often long and unstructured.\n","\n","2. **Summary** - Provides the corresponding concise and structured clinical summary, making it suitable for training supervised summarization models.\n"],"metadata":{"id":"QleaDsTqW7MC"}},{"cell_type":"markdown","metadata":{"id":"lnwETBOE6Bz5"},"source":["# **Installing and Importing Necessary Libraries**"]},{"cell_type":"code","source":["!pip install --no-deps \\\n","bitsandbytes==0.48.1 \\\n","accelerate==1.10.1 \\\n","xformers==0.0.32.post2 \\\n","peft==0.17.1 \\\n","trl==0.15.2 \\\n","triton==3.4.0 \\\n","cut-cross-entropy==25.1.1 \\\n","unsloth==2025.10.1 \\\n","unsloth_zoo==2025.10.1 \\\n","sentencepiece==0.2.1 \\\n","protobuf==5.29.5 \\\n","huggingface_hub==0.35.3 \\\n","hf_transfer==0.1.9 \\\n","transformers==4.51.3\n"],"metadata":{"id":"5EntJkwGEcR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q datasets==4.0.0 \\\n","evaluate==0.4.6 \\\n","bert-score==0.3.13"],"metadata":{"id":"WapwBjzhDwwK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note**:\n","- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n","- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."],"metadata":{"id":"mDp-EYZH-69E"}},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","import torch\n","import evaluate\n","from tqdm import tqdm\n","import pandas as pd\n","from datasets import Dataset\n","\n","from trl import SFTTrainer\n","from transformers import TrainingArguments, EarlyStoppingCallback, DataCollatorForSeq2Seq\n","from unsloth import is_bfloat16_supported"],"metadata":{"id":"tgtsNS-UrQg9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **1. Evaluation of LLM before FineTuning**"],"metadata":{"id":"YhyMoqjoq48A"}},{"cell_type":"markdown","source":["Before investing time and resources in fine-tuning, it is important to measure **how well the base model performs \"as is.\"**\n","This gives us a baseline score, which we will later compare against fine-tuned performance to demonstrate improvement."],"metadata":{"id":"cNcxHGuc5Jj2"}},{"cell_type":"markdown","source":["### **Loading the Testing Data**\n"],"metadata":{"id":"Vp4LNGG30e8w"}},{"cell_type":"code","source":["testing_data = pd.read_csv(\"/content/finetuning_medical_testing.csv\")           # Load medical test dataset containing dialogues and gold summaries\n","\n","test_dialogues = testing_data['conversation'].tolist()                          # dialogues (inputs)\n","test_summaries = testing_data['summary'].tolist()                               # human-written summaries (ground truth)"],"metadata":{"id":"Qn9T8qBZ5dQ1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Loading the Mistral Model**"],"metadata":{"id":"mHaWozqI0hmW"}},{"cell_type":"markdown","source":["We use the **Mistral-7B Instruct model** (7 billion parameter model optimized for instructions).\n","It is loaded with **4-bit quantization** - meaning it runs efficiently on limited GPU memory without losing much accuracy.\n"],"metadata":{"id":"sAdPYzVx9aaB"}},{"cell_type":"code","source":["# Load the instruction-tuned Mistral 7B model with 4-bit quantization\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",                     # model name\n","    max_seq_length=4096,                                                        # maximum sequence length\n","    dtype=None,                                                                 # auto-select data type\n","    load_in_4bit=True                                                           # load in 4-bit for memory efficiency\n",")"],"metadata":{"id":"HZWyFbFXsn_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FastLanguageModel.for_inference(model)"],"metadata":{"id":"N3VYp0p7C794"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Generate Summaries**\n"],"metadata":{"id":"b9sW9YYntYcs"}},{"cell_type":"markdown","source":["We now ask the model to summarize dialogues **without any fine-tuning**.\n","A custom prompt template is used to ensure the model follows instructions consistently."],"metadata":{"id":"E9GsCYhGB1EL"}},{"cell_type":"code","source":["alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Write a concise summary of the following dialogue.\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\n","\"\"\""],"metadata":{"id":"PHaAlmvZfTEp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will create a function to manage the entire prediction process, which includes the following steps:\n","1. Structure the Prompt\n","2. Tokenize the Prompt\n","3. Generate the Output from Tokens\n","4. Decode the Generated Output\n","\n","This function allows us to avoid code repetition and streamline the process. Whenever we need to test a model, we can simply call this function instead of rewriting these steps each time.\n"],"metadata":{"id":"5KLaPzHSUjIs"}},{"cell_type":"code","source":["def generate_summaries(dialogues, model, tokenizer, prompt_template, max_new_tokens=100):\n","    summaries = []\n","\n","    for dialogue in dialogues:\n","        # create a prompt for the dialogue\n","        prompt = prompt_template.format(dialogue,\" \")\n","\n","        # tokenize the prompt\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","        # generate the output\n","        outputs = model.generate(**inputs,\n","                                 max_new_tokens=max_new_tokens,\n","                                 pad_token_id=tokenizer.eos_token_id\n","                                 )\n","\n","        # decode: skip input tokens, keep only generated part\n","        input_len = inputs[\"input_ids\"].shape[-1]\n","        summary = tokenizer.decode(outputs[0][input_len:],\n","                                   skip_special_tokens=True,\n","                                   cleanup_tokenization_spaces=True\n","                                   )\n","\n","        summaries.append(summary.strip())\n","\n","    return summaries"],"metadata":{"id":"v5veapxHUcGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calling the function to generate summaries\n","predicted_summaries = generate_summaries(dialogues=test_dialogues,\n","                                         model=model,\n","                                         tokenizer=tokenizer,\n","                                         prompt_template=alpaca_prompt_template\n","                                         )"],"metadata":{"id":"9nJPYlXZSx80"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Evaluate Using BERTScore**\n"],"metadata":{"id":"kC8DIfb80ULd"}},{"cell_type":"markdown","source":["**BERTScore** is an evaluation metric for text generation tasks such as summarization, translation, and captioning. Unlike traditional metrics like ROUGE or BLEU that rely on exact word overlaps, BERTScore uses embeddings from a pre-trained BERT model to measure **semantic similarity** between the generated text (predictions) and the human-written text (references). This makes it more robust in capturing meaning, even when different words are used.\n","\n","* **Precision** - Measures how much of the content in the generated text is actually relevant to the reference. A high precision means the model is not adding irrelevant or “extra” information.\n","\n","* **Recall** - Measures how much of the important content from the reference is captured by the generated text. A high recall means the model covers most of the key points, even if it includes some extra details.\n","\n","* **F1 Score** - Combines both precision and recall into a balanced score. It shows how well the generated text both *covers the important content* and *stays relevant*. This is usually reported as the main metric for BERTScore.\n","\n","In short, BERTScore helps evaluate not just word matching but whether the **meaning** of the generated text aligns with the reference.\n"],"metadata":{"id":"FLHAna-S78Li"}},{"cell_type":"code","source":["bert_scorer = evaluate.load(\"bertscore\")                                        # Load BERTScore evaluation metric\n","\n","score = bert_scorer.compute(                                                    # Compute BERTScore between generated summaries and gold references\n","    predictions=predicted_summaries,                                            # model-generated summaries\n","    references=test_summaries,                                                  # ground-truth summaries\n","    lang='en',                                                                  # language\n","    rescale_with_baseline=True                                                  # normalize scores for fair comparison\n",")\n","\n","\n","baseline_score = sum(score['f1']) / len(score['f1'])                            # Compute the average F1 score across test set\n","print(baseline_score)                                                           # Print the average F1 score"],"metadata":{"id":"Z8KBgaUXDrY2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The BERT Score of Mistral LLM is 0.37**\n"],"metadata":{"id":"tvZ1hPLCWwYt"}},{"cell_type":"markdown","source":["# **2. FineTuning an LLM**"],"metadata":{"id":"AFBNy7vSq42O"}},{"cell_type":"markdown","source":["Now that we have measured the **baseline performance** of the Mistral-7B model, the next step is to **fine-tune the model on our medical dataset.**\n","\n","Fine-tuning helps the model specialize in summarizing medical dialogues, improving relevance and accuracy beyond its general-purpose training."],"metadata":{"id":"KEU3wx8jFUKP"}},{"cell_type":"markdown","source":["### **Load and Prepare Training Data**"],"metadata":{"id":"Km_LXL5DaGBO"}},{"cell_type":"markdown","source":["We begin with the **training dataset**, which contains doctor-patient conversations paired with gold-standard summaries.\n","\n","We then convert the dataset into a Hugging Face `Dataset` format for efficient handling."],"metadata":{"id":"xrek6S-FFlPg"}},{"cell_type":"code","source":["# Load training dataset\n","training = pd.read_csv(\"/content/finetuning_medical_training.csv\")\n","training_dict = training.to_dict(orient='list')\n","\n","# Create a dataset from the dictionary\n","training_dataset = Dataset.from_dict(training_dict)"],"metadata":{"id":"EjytqCN7rMBu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instruction-tuned prompt template for summarization\n","alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\""],"metadata":{"id":"pOAzduah_e_z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To make training effective, we format each example into this structure and append an EOS token to clearly signal the end of the response."],"metadata":{"id":"w-V22ZR9Hl0V"}},{"cell_type":"code","source":["EOS_TOKEN = tokenizer.eos_token"],"metadata":{"id":"_TURs0KUrL9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Format each training example into the Alpaca prompt structure\n","def prompt_formatter(example, prompt_template):\n","    instruction='Write a concise summary of the following dialogue.'\n","    dialogue=example[\"conversation\"]\n","    summary=example[\"summary\"]\n","\n","    formatted_prompt = prompt_template.format(instruction, dialogue, summary) + EOS_TOKEN\n","\n","    return {'text': formatted_prompt}"],"metadata":{"id":"v0sjC8_Y_e9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply formatting to training dataset\n","formatted_training_dataset = training_dataset.map(\n","    prompt_formatter,\n","    fn_kwargs={'prompt_template': alpaca_prompt}\n",")"],"metadata":{"id":"V74wlPa7_e71"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Load and Prepare Validation Data**"],"metadata":{"id":"mChybNltH-1-"}},{"cell_type":"markdown","source":["We also prepare a **validation dataset** (unseen during training).\n","This allows us to track the model's progress and prevent overfitting."],"metadata":{"id":"_j9eElqCID_R"}},{"cell_type":"code","source":["# Load validation dataset\n","validation=pd.read_csv(\"/content/finetuning_medical_validation.csv\")\n","validation_dict =validation.to_dict(orient='list')\n","\n","# Create a dataset from the dictionary\n","validation_dataset = Dataset.from_dict(validation_dict)"],"metadata":{"id":"jvnCfQ9I_e5F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply formatting to training dataset\n","formatted_validation_dataset = validation_dataset.map(\n","    prompt_formatter,\n","    fn_kwargs={'prompt_template': alpaca_prompt}\n",")"],"metadata":{"id":"U2wTZleC_e2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Apply Parameter-Efficient Fine-Tuning (LoRA)**"],"metadata":{"id":"wCmKWuZkBzzR"}},{"cell_type":"markdown","source":["We now patch in the adapter modules to the base model using the `get_peft_model` method."],"metadata":{"id":"AYRXt6WZBzlO"}},{"cell_type":"code","source":["# Apply LoRA fine-tuning (PEFT) to the base model\n","model = FastLanguageModel.get_peft_model(\n","    model,                                                                      # base model to fine-tune\n","    r=16,                                                                       # rank of LoRA matrices\n","    lora_alpha=16,                                                              # scaling factor for LoRA updates\n","    lora_dropout=0,                                                             # dropout (0 means no dropout)\n","    bias=\"none\",                                                                # don't fine-tune bias terms\n","    target_modules=[                                                            # layers where LoRA is applied\n","        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","        \"gate_proj\", \"up_proj\", \"down_proj\"\n","    ],\n","    use_gradient_checkpointing=True,                                            # saves memory by recomputing activations\n","    random_state=42,                                                            # reproducibility\n","    loftq_config=None                                                           # not using LoFTQ quantization here\n",")"],"metadata":{"id":"gnIsofeq_e0B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First, we load the model using `FastLanguageModel`. After initializing the base model, we patch the adapter models using `get_peft_model`. This allows us to enhance the model by integrating LoRA (Low-Rank Adaptation) weights, which are now displayed as part of the model's configuration. This setup facilitates improved customization and adaptability of the model for specialized tasks."],"metadata":{"id":"HQHqCggd2jYh"}},{"cell_type":"code","source":["model"],"metadata":{"id":"te1_HsoJB7Es"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Configure Training**"],"metadata":{"id":"vQ_Bz_NQKCtV"}},{"cell_type":"markdown","source":["We now define training arguments for **supervised fine-tuning (SFT)** using Hugging Face's `SFTTrainer`.\n","The setup ensures training runs efficiently on Colab GPUs."],"metadata":{"id":"T4UQ-4nPKF_g"}},{"cell_type":"code","source":["# Supervised Fine-Tuning (SFT) trainer setup\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = formatted_training_dataset,\n","    eval_dataset = formatted_validation_dataset,\n","    dataset_text_field = \"text\",                                                # column containing the text\n","    max_seq_length = 2048,                                                      # max sequence length\n","    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer),                # handles padding, batching\n","    dataset_num_proc = 2,                                                       # parallel dataset processing\n","    packing = False,                                                            # set True to pack short sequences (faster)\n","\n","    # Training hyperparameters\n","    args = TrainingArguments(\n","        per_device_train_batch_size = 2,                                        # batch size per GPU\n","        gradient_accumulation_steps = 4,                                        # simulate larger batch via accumulation\n","        warmup_steps = 5,                                                       # warmup before LR schedule\n","        max_steps = 60,                                                         # limit training steps (overrides epochs)\n","        learning_rate = 2e-4,                                                   # base learning rate\n","        fp16 = not is_bfloat16_supported(),                                     # use fp16 if bf16 not available\n","        bf16 = is_bfloat16_supported(),                                         # use bf16 if supported (better precision)\n","        logging_steps = 1,                                                      # log every step\n","        optim = \"adamw_8bit\",                                                   # memory-efficient optimizer\n","        weight_decay = 0.01,                                                    # weight decay for regularization\n","        lr_scheduler_type = \"linear\",                                           # linear learning rate decay\n","        seed = 3407,                                                            # reproducibility\n","        output_dir = \"outputs\",                                                 # save model outputs\n","        report_to = \"none\"                                                      # disable logging to external services\n","    ),\n",")\n"],"metadata":{"id":"0EGcJ09z_exb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Begin fine-tuning\n","training_history = trainer.train()"],"metadata":{"id":"xH8cGYJJ_0Wh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Save Fine-Tuned Model**\n"],"metadata":{"id":"C45mg7AjBMTr"}},{"cell_type":"markdown","source":["After training, we save the **LoRA fine-tuned model** for future inference."],"metadata":{"id":"tucu4Hp6LNVn"}},{"cell_type":"code","source":["# Save locally\n","lora_model_name = \"finetuned_mistral_llm\"\n","model.save_pretrained(lora_model_name)                                          # Save LoRA-finetuned model\n","\n","# Verify saved files\n","!ls -lh {lora_model_name}                                                       # List files inside the saved model folder"],"metadata":{"id":"QdJmXe09MEpH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Comment out this cell if you want to save the model to Drive\n","\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# drive_model_path = \"/content/drive/MyDrive/finetuned_mistral_llm\"\n","\n","# !cp -r {lora_model_name} {drive_model_path}"],"metadata":{"id":"XjPmezvToOeH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **3. Evaluation of LLM after FineTuning**"],"metadata":{"id":"-Sq_u6Mpq441"}},{"cell_type":"markdown","source":["Once the model has been fine-tuned, the next step is to **measure its effectiveness**. In business terms, this is like checking whether the training investment has paid off and whether the system can generate outputs that align with expectations."],"metadata":{"id":"ieQO8m5UMlEx"}},{"cell_type":"markdown","source":["### **Loading the Finetuned Mistral Model**"],"metadata":{"id":"qGV4lcz8FHa1"}},{"cell_type":"code","source":["finetuned_model, finetuned_tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=lora_model_name,                                                 # Replace the model name with \"drive_model_path\" if you are loading the model from Drive\n","    max_seq_length=2048,\n","    dtype=None,\n","    load_in_4bit=True\n",")"],"metadata":{"id":"dXDLid2b0zLu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Note** <br> It is not strictly necessary to save and reload the model if you are evaluating it in the same Colab session - the model is still in memory. **However, as a best practice,** we save the fine-tuned model and then reload it for evaluation."],"metadata":{"id":"OaJ5bmUoN6c_"}},{"cell_type":"markdown","source":["### **Generate Summaries**"],"metadata":{"id":"Fb0DfllzFFmx"}},{"cell_type":"code","source":["# Calling the function to generate summaries\n","predicted_summaries = generate_summaries(dialogues=test_dialogues,\n","                                         model=finetuned_model,\n","                                         tokenizer=finetuned_tokenizer,\n","                                         prompt_template=alpaca_prompt_template\n","                                         )"],"metadata":{"id":"Q1wHlgN7VEbh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Evaluate Using BERTScore**"],"metadata":{"id":"3WvP1kbtFIcF"}},{"cell_type":"code","source":["score = bert_scorer.compute(\n","    predictions=predicted_summaries,\n","    references=test_summaries,\n","    lang='en',\n","    rescale_with_baseline=True\n",")\n","finetune_score = sum(score['f1']) / len(score['f1'])\n","print(finetune_score)"],"metadata":{"id":"jTPGlwJP1llz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The BERT Score of Finetuned Mistral LLM is 0.67**\n","\n","- This suggests that the model is producing reasonably good summaries that align well with human expectations.\n","\n","- While not perfect, it demonstrates that fine-tuning has meaningfully improved the model's ability to perform the summarization task."],"metadata":{"id":"qFbvHrEQjsDb"}},{"cell_type":"markdown","source":["# **Conclusion**\n"],"metadata":{"id":"JPgLSo07YBo3"}},{"cell_type":"markdown","source":["* The fine-tuned model generates concise and clinically accurate summaries, enhancing the quality of documentation and supporting improved patient care.\n","\n","* Summaries now align closely with clinical communication needs, allowing medical practitioners quick access to relevant and structured information.\n","\n","* This approach establishes a foundation for developing intelligent healthcare support systems that are context-aware, scalable, and tailored to the needs of medical professionals.\n","\n","* The system's effectiveness is contingent on the availability of high-quality task-relevant consultation data and may require further refinement for diverse healthcare contexts.\n","\n","* While the model produces clinically relevant summaries, continuous evaluation and possible adjustments are necessary to maintain alignment with evolving healthcare standards and practices."],"metadata":{"id":"hYSz_oGD5U6e"}},{"cell_type":"markdown","metadata":{"id":"mQvaNDqQ3BJa"},"source":["<font size = 6 color=\"#4682B4\"><b> Power Ahead </font>\n","___"]}]}