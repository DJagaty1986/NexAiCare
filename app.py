"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  NexAiCare - AI-Powered Healthcare Platform                            â•‘
â•‘  Developed by Jade Global                                              â•‘
â•‘  Six Integrated AI Modules in One Unified Application                  â•‘
â•‘  Version: 1.0.0                                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPORTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import streamlit as st
import os
import json
import sqlite3
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
from datetime import datetime
import time
import base64
import warnings

from openai import OpenAI

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import Tool
from langchain_classic.agents import initialize_agent
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_community.utilities.sql_database import SQLDatabase
from langchain_community.agent_toolkits import create_sql_agent

warnings.filterwarnings("ignore")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PAGE CONFIG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.set_page_config(
    page_title="NexAiCare - Healthcare AI Platform",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PATHS & CONSTANTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BASE_DIR = Path(__file__).parent
DB_PATH = str(BASE_DIR / "hc_data.db")
PDF_PATH = str(BASE_DIR / "medical_diagnosis_manual.pdf")
CSV_PATH = str(BASE_DIR / "finetuning_medical_testing.csv")
VECTOR_DB_DIR = str(BASE_DIR / "Healthcare_db")
LOGO_DARK_PATH = BASE_DIR / "Jade_Global_Logo_Dark_Mode.PNG"
LOGO_LIGHT_PATH = BASE_DIR / "Jade_Global_Logo_Light_Mode.PNG"

MENU_ITEMS = {
    "ğŸ“Š Dashboard": "dashboard",
    "ğŸ©º Medical Assistant AI": "module_1",
    "ğŸ” Claim Audit AI": "module_2",
    "ğŸ“ Consultation Notes AI": "module_3",
    "ğŸ›¡ï¸ PII/PHI Monitor": "module_4",
    "ğŸ’» Medical Coding AI": "module_5",
    "ğŸ”¬ Clinical Diagnostic AI": "module_6",
    "âš™ï¸ Settings": "settings",
}

STATUS_COLORS = {"Paid": "#28a745", "Denied": "#dc3545", "Pending": "#ffc107"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CUSTOM CSS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def inject_css():
    st.markdown(
        """
    <style>
        /* â”€â”€ Global â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .main .block-container {padding: 1.2rem 2rem 2rem 2rem; max-width:100%;}
        h1, h2, h3 {color: #0A1628;}

        /* â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        section[data-testid="stSidebar"] {
            background: linear-gradient(180deg, #0A1628 0%, #142d4c 100%);
        }
        section[data-testid="stSidebar"] * {color: rgba(255,255,255,0.85) !important;}
        section[data-testid="stSidebar"] hr {border-color: rgba(255,255,255,0.15);}
        section[data-testid="stSidebar"] .stRadio > div > label {
            padding: 9px 14px; border-radius: 8px; margin: 1px 0;
            font-size: 14.5px; transition: background .2s;
        }
        section[data-testid="stSidebar"] .stRadio > div > label:hover {
            background: rgba(255,255,255,0.08);
        }
        section[data-testid="stSidebar"] .stRadio > div > label[data-checked="true"],
        section[data-testid="stSidebar"] .stRadio > div [data-testid="stMarkdownContainer"] {
            font-weight: 500;
        }

        /* â”€â”€ KPI Metric Cards â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .kpi-card {
            background: #ffffff; border-radius: 12px;
            padding: 20px 22px; text-align: center;
            box-shadow: 0 2px 12px rgba(0,0,0,0.06);
            border-left: 4px solid #FF6B35;
            transition: transform .15s, box-shadow .15s;
        }
        .kpi-card:hover {transform: translateY(-2px); box-shadow: 0 6px 20px rgba(0,0,0,0.10);}
        .kpi-value {font-size: 28px; font-weight: 700; color: #0A1628; margin: 4px 0;}
        .kpi-label {font-size: 13px; color: #6c757d; text-transform: uppercase; letter-spacing: .8px;}
        .kpi-card.green  {border-left-color: #28a745;}
        .kpi-card.red    {border-left-color: #dc3545;}
        .kpi-card.blue   {border-left-color: #17a2b8;}
        .kpi-card.orange {border-left-color: #FF6B35;}

        /* â”€â”€ Insight Cards â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .insight-card {
            background: #fff; border-radius: 10px; padding: 18px 22px;
            margin: 8px 0; box-shadow: 0 1px 8px rgba(0,0,0,0.05);
            border-left: 4px solid #17a2b8;
        }
        .insight-card .badge {
            display: inline-block; padding: 3px 10px; border-radius: 12px;
            font-size: 11px; font-weight: 600; margin-right: 8px;
        }
        .badge-low  {background:#e8f5e9; color:#2e7d32;}
        .badge-med  {background:#fff3e0; color:#e65100;}
        .badge-high {background:#ffebee; color:#c62828;}
        .badge-info {background:#e3f2fd; color:#1565c0;}

        /* â”€â”€ Page Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .page-header {
            background: linear-gradient(135deg, #0A1628, #1B3A5C);
            color: #fff; padding: 28px 32px; border-radius: 14px;
            margin-bottom: 24px;
        }
        .page-header h2 {color: #fff !important; margin:0 0 6px 0; font-size:26px;}
        .page-header p  {color: rgba(255,255,255,0.75); margin:0; font-size:14.5px;}

        /* â”€â”€ Chat Messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .stChatMessage {border-radius: 12px !important;}

        /* â”€â”€ Buttons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .stButton > button {
            border-radius: 8px; font-weight: 500;
            transition: all .2s;
        }
        div.stButton > button:first-child {
            background-color: #FF6B35; color: white; border: none;
        }
        div.stButton > button:first-child:hover {
            background-color: #e85d2c; color: white;
        }

        /* â”€â”€ Comingâ€‘soon Banner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .coming-soon {
            text-align: center; padding: 80px 40px;
            background: linear-gradient(135deg, #f8f9fa, #e9ecef);
            border-radius: 16px; margin: 30px 0;
        }
        .coming-soon h2 {font-size: 36px; color: #1B3A5C;}
        .coming-soon p  {font-size: 16px; color: #6c757d;}

        /* â”€â”€ Sample chip / tag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .sample-chip {
            display: inline-block; background: #f0f2f6;
            padding: 6px 14px; border-radius: 20px; margin: 4px;
            font-size: 13px; color: #333; cursor: pointer;
            border: 1px solid #ddd; transition: background .15s;
        }
        .sample-chip:hover {background: #e2e6ea;}

        /* â”€â”€ Copyright â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .copyright {
            font-size: 11px; color: rgba(255,255,255,0.45);
            text-align: center; padding: 12px 0 8px 0;
            border-top: 1px solid rgba(255,255,255,0.1);
            margin-top: 20px;
        }

        /* â”€â”€ Status badges for tables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        .status-paid   {color:#28a745; font-weight:600;}
        .status-denied {color:#dc3545; font-weight:600;}
        .status-pending{color:#ffc107; font-weight:600;}

        /* â”€â”€ Hide Streamlit branding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        header {visibility: hidden;}
    </style>""",
        unsafe_allow_html=True,
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SESSION STATE INITIALIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def init_session_state():
    defaults = {
        "openai_api_key": os.environ.get("OPENAI_API_KEY", ""),
        "openai_api_base": os.environ.get("OPENAI_API_BASE", ""),
        "selected_page": "ğŸ“Š Dashboard",
        "vectorstore_ready": False,
        "module1_chat_history": [],
        "module2_chat_history": [],
        "module2_context_memory": "",
        "module3_history": [],
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UTILITY HELPERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def get_api_key():
    """Return OpenAI API key from session state, secrets, or env."""
    if st.session_state.get("openai_api_key"):
        return st.session_state["openai_api_key"]
    try:
        return st.secrets["OPENAI_API_KEY"]
    except Exception:
        return os.environ.get("OPENAI_API_KEY", "")


def get_api_base():
    if st.session_state.get("openai_api_base"):
        return st.session_state["openai_api_base"]
    try:
        return st.secrets.get("OPENAI_API_BASE", "")
    except Exception:
        return os.environ.get("OPENAI_API_BASE", "")


def get_openai_client():
    key = get_api_key()
    if not key:
        return None
    base = get_api_base()
    kwargs = {"api_key": key}
    if base:
        kwargs["base_url"] = base
    return OpenAI(**kwargs)


def get_langchain_llm(temperature=0, model="gpt-4o-mini"):
    key = get_api_key()
    base = get_api_base()
    kwargs = {"model_name": model, "temperature": temperature, "openai_api_key": key}
    if base:
        kwargs["openai_api_base"] = base
    return ChatOpenAI(**kwargs)


def api_key_configured():
    return bool(get_api_key())


def render_page_header(icon, title, description):
    st.markdown(
        f"""<div class="page-header">
            <h2>{icon}&nbsp; {title}</h2>
            <p>{description}</p>
        </div>""",
        unsafe_allow_html=True,
    )


def render_kpi_card(label, value, color_class="orange"):
    st.markdown(
        f"""<div class="kpi-card {color_class}">
            <div class="kpi-label">{label}</div>
            <div class="kpi-value">{value}</div>
        </div>""",
        unsafe_allow_html=True,
    )


def show_api_warning():
    st.warning("Please configure your OpenAI API key in **âš™ï¸ Settings** to use this module.")


def load_logo_b64(path):
    """Return base64-encoded image if the logo file exists."""
    if Path(path).exists():
        data = Path(path).read_bytes()
        return base64.b64encode(data).decode()
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA LOADING (cached)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@st.cache_data(ttl=300)
def load_claims_data():
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query("SELECT * FROM claims", conn)
    conn.close()
    return df


@st.cache_data
def load_test_conversations():
    return pd.read_csv(CSV_PATH)


def compute_kpis(df):
    total_claims = len(df)
    total_amount = df["claim_amount"].sum()
    denial_rate = (df["claim_status"] == "Denied").mean() * 100
    avg_los = df["length_of_stay"].mean()
    audit_flag_rate = df["coding_audit_flag"].mean() * 100
    readmission_rate = df["readmission_within_30d"].mean() * 100
    return {
        "total_claims": total_claims,
        "total_amount": total_amount,
        "denial_rate": denial_rate,
        "avg_los": avg_los,
        "audit_flag_rate": audit_flag_rate,
        "readmission_rate": readmission_rate,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MODULE 1 â€” MEDICAL ASSISTANT AI  (RAG)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RAG_SYSTEM_PROMPT = """You are a medical knowledge assistant. Your task is to answer healthcare questions using ONLY the provided context.

Rules:
- Answer strictly from the ###Context below.
- If the answer cannot be derived from the context, respond: "I don't have enough information in the knowledge base to answer this question."
- Be concise, professional, and clinically accurate.
- Do not mention the context or its source in your response.
"""

RAG_USER_TEMPLATE = """
###Context
{context}

###Question
{question}
"""


@st.cache_resource(show_spinner=False)
def build_vector_store(_api_key, _api_base):
    """Load PDF, chunk, embed, and persist to ChromaDB."""
    kwargs = {"openai_api_key": _api_key, "chunk_size": 512}
    if _api_base:
        kwargs["openai_api_base"] = _api_base
    embedding_model = OpenAIEmbeddings(**kwargs)

    # Check for existing vector store
    if os.path.exists(VECTOR_DB_DIR) and os.listdir(VECTOR_DB_DIR):
        return Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embedding_model)

    loader = PyMuPDFLoader(PDF_PATH)
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        encoding_name="cl100k_base", chunk_size=512, chunk_overlap=20
    )
    chunks = loader.load_and_split(splitter)
    vectorstore = Chroma.from_documents(
        chunks, embedding_model, persist_directory=VECTOR_DB_DIR
    )
    return vectorstore


def get_rag_response(question, vectorstore, client, k=5, max_tokens=500):
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": k})
    relevant_docs = retriever.invoke(question)
    context = ". ".join([d.page_content for d in relevant_docs])

    user_msg = RAG_USER_TEMPLATE.format(context=context, question=question)
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": RAG_SYSTEM_PROMPT},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=max_tokens,
            temperature=0.3,
            top_p=0.95,
        )
        answer = resp.choices[0].message.content.strip()
    except Exception as e:
        answer = f"Sorry, I encountered an error: {e}"
    sources = list({d.metadata.get("page", "N/A") for d in relevant_docs})
    return answer, sources


def render_module_1():
    render_page_header(
        "ğŸ©º",
        "Medical Assistant AI",
        "RAG-powered knowledge base for clinical decision support â€” backed by the Merck Medical Manual (4 000+ pages).",
    )
    if not api_key_configured():
        show_api_warning()
        return

    # Initialize vector store
    client = get_openai_client()
    if not st.session_state.vectorstore_ready:
        if not os.path.exists(PDF_PATH):
            st.error("Medical manual PDF not found. Please place `medical_diagnosis_manual.pdf` in the project root.")
            return
        st.info("The knowledge base needs to be initialized from the medical manual. This may take several minutes on first run.")
        if st.button("Initialize Knowledge Base", key="init_kb"):
            with st.spinner("Loading PDF, chunking, and creating embeddings â€” please wait..."):
                try:
                    build_vector_store(get_api_key(), get_api_base())
                    st.session_state.vectorstore_ready = True
                    st.rerun()
                except Exception as e:
                    st.error(f"Failed to initialize: {e}")
        return

    vectorstore = build_vector_store(get_api_key(), get_api_base())

    # Sample questions
    st.markdown("##### Try a sample question")
    sample_qs = [
        "What is the protocol for managing sepsis in a critical care unit?",
        "What are the common symptoms for appendicitis?",
        "What treatments are recommended for traumatic brain injury?",
        "What are the effective treatments for alopecia areata?",
    ]
    cols = st.columns(2)
    for i, q in enumerate(sample_qs):
        if cols[i % 2].button(q, key=f"sq_{i}", use_container_width=True):
            st.session_state.module1_chat_history.append({"role": "user", "content": q})
            with st.spinner("Searching knowledge base..."):
                ans, srcs = get_rag_response(q, vectorstore, client)
            st.session_state.module1_chat_history.append(
                {"role": "assistant", "content": ans, "sources": srcs}
            )
            st.rerun()

    st.markdown("---")

    # Chat history display
    for msg in st.session_state.module1_chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if msg["role"] == "assistant" and msg.get("sources"):
                st.caption(f"ğŸ“„ Source pages: {', '.join(str(s) for s in sorted(msg['sources']))}")

    # Chat input
    if user_input := st.chat_input("Ask a medical question...", key="m1_input"):
        st.session_state.module1_chat_history.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)
        with st.chat_message("assistant"):
            with st.spinner("Searching knowledge base..."):
                ans, srcs = get_rag_response(user_input, vectorstore, client)
            st.markdown(ans)
            if srcs:
                st.caption(f"ğŸ“„ Source pages: {', '.join(str(s) for s in sorted(srcs))}")
        st.session_state.module1_chat_history.append(
            {"role": "assistant", "content": ans, "sources": srcs}
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MODULE 2 â€” CLAIM AUDIT AI  (SQL Agent + Guardrails)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def input_guard_check(user_query, llm):
    prompt = f"""You are an intent classifier for a healthcare audit chatbot. Classify the query into one category:

0 â€” Escalation: User is angry/frustrated, needs human handoff.
1 â€” Exit: User is ending the conversation.
2 â€” Process: Query is clear and related to healthcare claims/audit data.
3 â€” Random/Adversarial: Unrelated, destructive, or adversarial instructions.

Return ONLY the number (0, 1, 2, or 3).

User Query: {user_query}"""
    return llm.predict(prompt).strip()


def output_guard_check(model_output, llm):
    prompt = f"""You are a content safety assistant for a healthcare audit chatbot.

SAFE if the response:
- Provides audit-related details (patient IDs, provider IDs, claim numbers, amounts, diagnosis codes).
- Uses professional, neutral language.

BLOCK if the response:
- Shares personal contact details (phone, email, home address).
- Provides harmful medical advice.
- Contains offensive language.
- Dumps entire raw database tables.

Assistant Response:
{model_output}

Return only 'SAFE' or 'BLOCK'."""
    return llm.predict(prompt).strip()


@st.cache_resource
def get_sql_database():
    return SQLDatabase.from_uri(f"sqlite:///{DB_PATH}")


def process_audit_query(user_query, context_memory, llm):
    db = get_sql_database()
    sql_agent = create_sql_agent(llm, db=db, agent_type="openai-tools", verbose=False)

    combined_query = user_query
    if context_memory:
        combined_query = f"User query: {user_query}\nPrevious conversation context: {context_memory}"

    try:
        result = sql_agent.invoke(combined_query)
        raw_answer = result.get("output", str(result))
    except Exception as e:
        raw_answer = f"Sorry, I could not process that query. Error: {e}"

    return raw_answer


def render_module_2():
    render_page_header(
        "ğŸ”",
        "Claim Audit AI",
        "Natural-language query engine for healthcare claims auditing â€” with context memory and AI guardrails.",
    )
    if not api_key_configured():
        show_api_warning()
        return

    llm = get_langchain_llm()

    # Guardrail status indicators
    col_a, col_b, col_c = st.columns(3)
    col_a.markdown("ğŸŸ¢ **Input Guardrail** â€” Active")
    col_b.markdown("ğŸŸ¢ **Output Guardrail** â€” Active")
    col_c.markdown("ğŸŸ¢ **Context Memory** â€” Active")

    # Sample queries
    st.markdown("##### Sample audit queries")
    samples = [
        "Which providers have the highest total claim amount?",
        "What was the denial reason for claim CLM1043?",
        "Which specialty has the longest average length of stay?",
        "How many claims are flagged for coding audit?",
    ]
    scols = st.columns(2)
    for i, sq in enumerate(samples):
        if scols[i % 2].button(sq, key=f"aq_{i}", use_container_width=True):
            st.session_state.module2_chat_history.append({"role": "user", "content": sq})
            with st.spinner("Processing audit query..."):
                guard_res = input_guard_check(sq, llm)
                if guard_res == "2":
                    raw = process_audit_query(sq, st.session_state.module2_context_memory, llm)
                    safety = output_guard_check(raw, llm)
                    if safety == "BLOCK":
                        raw = "I'm sorry, but I cannot provide the requested information. Your request is being forwarded to the compliance team."
                    st.session_state.module2_context_memory += f"\nuser: {sq}\nassistant: {raw}"
                else:
                    raw = _guardrail_response(guard_res)
            st.session_state.module2_chat_history.append({"role": "assistant", "content": raw})
            st.rerun()

    st.markdown("---")

    # Chat display
    for msg in st.session_state.module2_chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # Chat input
    if user_input := st.chat_input("Ask an audit question...", key="m2_input"):
        st.session_state.module2_chat_history.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)
        with st.chat_message("assistant"):
            with st.spinner("Analyzing..."):
                guard_res = input_guard_check(user_input, llm)
                if guard_res == "2":
                    raw = process_audit_query(user_input, st.session_state.module2_context_memory, llm)
                    safety = output_guard_check(raw, llm)
                    if safety == "BLOCK":
                        raw = "I'm sorry, but I cannot provide the requested information. Your request is being forwarded to the compliance team."
                    st.session_state.module2_context_memory += f"\nuser: {user_input}\nassistant: {raw}"
                else:
                    raw = _guardrail_response(guard_res)
            st.markdown(raw)
        st.session_state.module2_chat_history.append({"role": "assistant", "content": raw})


def _guardrail_response(code):
    responses = {
        "0": "I understand your frustration. Your request is being escalated to the internal compliance team for immediate assistance.",
        "1": "Thank you for using Claim Audit AI! I hope I was able to help with your query.",
        "3": "Apologies, I can only assist with questions about healthcare audits and claims. Inquiries outside this scope cannot be addressed.",
    }
    return responses.get(code, "We are experiencing a technical issue. Please try again.")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MODULE 3 â€” CONSULTATION NOTES AI  (Summarization)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SUMMARY_SYSTEM_PROMPT = """You are a clinical documentation assistant trained to generate structured summaries from doctor-patient conversations.

Instructions:
- Read the conversation carefully.
- Output a concise clinical summary in this exact format:
  **Patient Concern:** <main symptoms/complaints>
  **Findings:** <doctor's observations and diagnosis>
  **Action Plan:** <prescriptions, tests ordered, referrals, follow-ups>
- Use professional medical terminology.
- Do NOT add information not present in the conversation.
- Keep the summary to 3-5 sentences total.
"""


def generate_clinical_summary(conversation, client):
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": SUMMARY_SYSTEM_PROMPT},
                {"role": "user", "content": f"Summarize this consultation:\n\n{conversation}"},
            ],
            max_tokens=400,
            temperature=0.2,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"Error generating summary: {e}"


def render_module_3():
    render_page_header(
        "ğŸ“",
        "Consultation Notes AI",
        "AI-powered transcription and structured summaries of doctor-patient consultations.",
    )
    if not api_key_configured():
        show_api_warning()
        return

    client = get_openai_client()
    test_data = load_test_conversations()

    left, right = st.columns([1, 1])

    with left:
        st.markdown("#### Input Conversation")
        st.markdown(
            "Paste a doctor-patient dialogue below, or select a sample conversation."
        )

        # Sample selector
        sample_idx = st.selectbox(
            "Load a sample conversation",
            options=["â€” Select â€”"] + [f"Sample {i+1}" for i in range(len(test_data))],
            key="sample_conv",
        )

        default_text = ""
        if sample_idx != "â€” Select â€”":
            idx = int(sample_idx.split(" ")[1]) - 1
            default_text = test_data.iloc[idx]["conversation"]

        conversation_text = st.text_area(
            "Doctor-Patient Conversation",
            value=default_text,
            height=300,
            placeholder="Doctor: How are you feeling today?\nPatient: ...",
            key="conv_input",
        )

        generate_btn = st.button("Generate Summary", key="gen_summary", use_container_width=True)

    with right:
        st.markdown("#### Generated Clinical Summary")

        if generate_btn and conversation_text.strip():
            with st.spinner("Generating structured clinical summary..."):
                summary = generate_clinical_summary(conversation_text, client)
            st.session_state.module3_history.append(
                {"conversation": conversation_text[:100] + "...", "summary": summary}
            )
            st.markdown(summary)

            # Show reference summary if a sample was selected
            if sample_idx != "â€” Select â€”":
                idx = int(sample_idx.split(" ")[1]) - 1
                ref = test_data.iloc[idx]["summary"]
                with st.expander("ğŸ“‹ Reference Summary (ground truth)"):
                    st.info(ref)

        elif generate_btn:
            st.warning("Please enter a conversation to summarize.")
        else:
            st.markdown(
                '<p style="color:#999; padding:60px 0; text-align:center;">Summary will appear here after you click <b>Generate Summary</b>.</p>',
                unsafe_allow_html=True,
            )

    # History table
    if st.session_state.module3_history:
        st.markdown("---")
        st.markdown("#### Recent Summaries")
        hist_df = pd.DataFrame(st.session_state.module3_history)
        st.dataframe(hist_df, use_container_width=True, hide_index=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  DASHBOARD
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def render_dashboard():
    render_page_header(
        "ğŸ“Š",
        "Healthcare Operations Dashboard",
        "Real-time analytics and AI-powered insights across claims, compliance, and clinical operations.",
    )

    df = load_claims_data()
    kpis = compute_kpis(df)

    # â”€â”€ Row 1: KPI Cards â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        render_kpi_card("Total Claims", f"{kpis['total_claims']}", "blue")
    with c2:
        render_kpi_card("Total Billed", f"${kpis['total_amount']:,.2f}", "orange")
    with c3:
        render_kpi_card("Denial Rate", f"{kpis['denial_rate']:.1f}%", "red")
    with c4:
        render_kpi_card("Avg Length of Stay", f"{kpis['avg_los']:.1f} days", "green")

    st.markdown("<br>", unsafe_allow_html=True)

    # â”€â”€ Row 2: Charts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ch1, ch2 = st.columns(2)

    with ch1:
        status_counts = df["claim_status"].value_counts().reset_index()
        status_counts.columns = ["Status", "Count"]
        fig1 = px.pie(
            status_counts,
            values="Count",
            names="Status",
            hole=0.45,
            color="Status",
            color_discrete_map=STATUS_COLORS,
            title="Claims by Status",
        )
        fig1.update_layout(
            margin=dict(t=40, b=20, l=20, r=20),
            legend=dict(orientation="h", y=-0.1),
            font=dict(size=13),
        )
        st.plotly_chart(fig1, use_container_width=True)

    with ch2:
        dept_counts = df["department"].value_counts().reset_index()
        dept_counts.columns = ["Department", "Count"]
        fig2 = px.bar(
            dept_counts.sort_values("Count"),
            x="Count",
            y="Department",
            orientation="h",
            title="Claims by Department",
            color_discrete_sequence=["#1B3A5C"],
        )
        fig2.update_layout(
            margin=dict(t=40, b=20, l=20, r=20),
            yaxis_title="",
            xaxis_title="Number of Claims",
            font=dict(size=13),
        )
        st.plotly_chart(fig2, use_container_width=True)

    # â”€â”€ Row 3: More Charts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ch3, ch4 = st.columns(2)

    with ch3:
        dept_amount = (
            df.groupby(["department", "claim_status"])["claim_amount"]
            .sum()
            .reset_index()
        )
        fig3 = px.bar(
            dept_amount,
            x="department",
            y="claim_amount",
            color="claim_status",
            title="Claim Amount by Department & Status",
            color_discrete_map=STATUS_COLORS,
            barmode="stack",
        )
        fig3.update_layout(
            margin=dict(t=40, b=20, l=20, r=20),
            xaxis_title="",
            yaxis_title="Amount ($)",
            legend_title="Status",
            font=dict(size=13),
        )
        st.plotly_chart(fig3, use_container_width=True)

    with ch4:
        top_providers = (
            df.groupby("provider_id")["claim_amount"]
            .sum()
            .nlargest(10)
            .reset_index()
        )
        fig4 = px.bar(
            top_providers,
            x="claim_amount",
            y="provider_id",
            orientation="h",
            title="Top 10 Providers by Claim Amount",
            color_discrete_sequence=["#FF6B35"],
        )
        fig4.update_layout(
            margin=dict(t=40, b=20, l=20, r=20),
            yaxis_title="",
            xaxis_title="Total Amount ($)",
            font=dict(size=13),
        )
        st.plotly_chart(fig4, use_container_width=True)

    # â”€â”€ Row 4: AI-Powered Insights â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### ğŸ’¡ AI-Powered Insights")

    # Compute insights
    denied_df = df[df["claim_status"] == "Denied"]
    top_denial = denied_df["denial_reason"].value_counts()
    doc_incomplete = (df["documentation_complete"] == "No").mean() * 100
    consent_missing = (df["consent_on_file"] == "No").mean() * 100

    ins1, ins2 = st.columns(2)
    with ins1:
        severity = "badge-high" if kpis["denial_rate"] > 25 else ("badge-med" if kpis["denial_rate"] > 15 else "badge-low")
        st.markdown(
            f"""<div class="insight-card">
                <span class="badge {severity}">{"high" if "high" in severity else ("medium" if "med" in severity else "low")}</span>
                <span class="badge badge-info">compliance</span>
                <h4 style="margin:10px 0 4px 0;">Denial Rate Analysis</h4>
                <p style="margin:0; font-size:14px; color:#555;">
                    Current denial rate is <b>{kpis['denial_rate']:.1f}%</b>.
                    Top reason: <b>{top_denial.index[0] if len(top_denial) > 0 else 'N/A'}</b>
                    ({top_denial.values[0] if len(top_denial) > 0 else 0} claims).<br>
                    <b>Recommendation:</b> Review documentation workflows to reduce denials due to missing paperwork.
                </p>
            </div>""",
            unsafe_allow_html=True,
        )

    with ins2:
        sev2 = "badge-high" if doc_incomplete > 30 else ("badge-med" if doc_incomplete > 15 else "badge-low")
        st.markdown(
            f"""<div class="insight-card">
                <span class="badge {sev2}">{"high" if "high" in sev2 else ("medium" if "med" in sev2 else "low")}</span>
                <span class="badge badge-info">documentation</span>
                <h4 style="margin:10px 0 4px 0;">Documentation Compliance</h4>
                <p style="margin:0; font-size:14px; color:#555;">
                    <b>{doc_incomplete:.1f}%</b> of claims have incomplete documentation.
                    <b>{consent_missing:.1f}%</b> missing patient consent.<br>
                    <b>Recommendation:</b> Implement pre-submission checklists to ensure completeness.
                </p>
            </div>""",
            unsafe_allow_html=True,
        )

    ins3, ins4 = st.columns(2)
    with ins3:
        sev3 = "badge-med" if kpis["audit_flag_rate"] > 20 else "badge-low"
        st.markdown(
            f"""<div class="insight-card">
                <span class="badge {sev3}">{"medium" if "med" in sev3 else "low"}</span>
                <span class="badge badge-info">audit</span>
                <h4 style="margin:10px 0 4px 0;">Coding Audit Flags</h4>
                <p style="margin:0; font-size:14px; color:#555;">
                    <b>{kpis['audit_flag_rate']:.1f}%</b> of claims flagged for coding audit.
                    Review flagged claims to identify systematic coding issues.<br>
                    <b>Recommendation:</b> Conduct targeted training for high-flag departments.
                </p>
            </div>""",
            unsafe_allow_html=True,
        )

    with ins4:
        sev4 = "badge-med" if kpis["readmission_rate"] > 15 else "badge-low"
        st.markdown(
            f"""<div class="insight-card">
                <span class="badge {sev4}">{"medium" if "med" in sev4 else "low"}</span>
                <span class="badge badge-info">clinical</span>
                <h4 style="margin:10px 0 4px 0;">30-Day Readmission Rate</h4>
                <p style="margin:0; font-size:14px; color:#555;">
                    <b>{kpis['readmission_rate']:.1f}%</b> readmission within 30 days.
                    This metric impacts quality scores and reimbursement.<br>
                    <b>Recommendation:</b> Enhance discharge planning and follow-up protocols.
                </p>
            </div>""",
            unsafe_allow_html=True,
        )

    # â”€â”€ Row 5: Data Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown("### ğŸ“‹ Claims Overview")

    # Filters
    fc1, fc2, fc3 = st.columns(3)
    with fc1:
        dept_filter = st.multiselect(
            "Department", options=sorted(df["department"].unique()), default=[]
        )
    with fc2:
        status_filter = st.multiselect(
            "Claim Status", options=sorted(df["claim_status"].unique()), default=[]
        )
    with fc3:
        amount_range = st.slider(
            "Claim Amount Range",
            min_value=float(df["claim_amount"].min()),
            max_value=float(df["claim_amount"].max()),
            value=(float(df["claim_amount"].min()), float(df["claim_amount"].max())),
        )

    filtered = df.copy()
    if dept_filter:
        filtered = filtered[filtered["department"].isin(dept_filter)]
    if status_filter:
        filtered = filtered[filtered["claim_status"].isin(status_filter)]
    filtered = filtered[
        (filtered["claim_amount"] >= amount_range[0])
        & (filtered["claim_amount"] <= amount_range[1])
    ]

    display_cols = [
        "claim_id", "department", "provider_id", "diagnosis_code",
        "procedure_code", "claim_amount", "claim_status", "denial_reason",
        "length_of_stay", "admission_date", "discharge_date",
    ]
    st.dataframe(
        filtered[display_cols].style.format({"claim_amount": "${:,.2f}"}),
        use_container_width=True,
        height=400,
        hide_index=True,
    )
    st.caption(f"Showing {len(filtered)} of {len(df)} claims")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  COMING SOON PAGES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMING_SOON_INFO = {
    "module_4": {
        "icon": "ğŸ›¡ï¸",
        "title": "PII/PHI Monitor",
        "desc": "AI-enabled continuous monitoring for HIPAA, GDPR, and CCPA compliance. Automatically detect and flag Protected Health Information (PHI) and Personally Identifiable Information (PII) across all data pipelines.",
        "features": ["Real-time PII/PHI scanning", "HIPAA compliance dashboard", "Automated redaction engine", "Audit trail & reporting"],
    },
    "module_5": {
        "icon": "ğŸ’»",
        "title": "Medical Coding AI",
        "desc": "Automated CPT/ICD coding suggestions with audit trails from Electronic Health Records (EHR). Reduce coding errors and accelerate the revenue cycle.",
        "features": ["CPT/ICD-10 auto-coding", "Coding accuracy analytics", "DRG optimization", "Coder productivity metrics"],
    },
    "module_6": {
        "icon": "ğŸ”¬",
        "title": "Clinical Diagnostic AI",
        "desc": "Medical imaging captioning for X-ray, CT, and MRI with differential diagnosis suggestions. Assist radiologists with AI-powered second opinions.",
        "features": ["X-ray/CT/MRI analysis", "Differential diagnosis", "DICOM integration", "Radiologist workflow support"],
    },
}


def render_coming_soon(module_key):
    info = COMING_SOON_INFO[module_key]
    render_page_header(info["icon"], info["title"], info["desc"])

    st.markdown(
        f"""<div class="coming-soon">
            <h2>ğŸš€ Coming Soon</h2>
            <p>This module is part of <b>Phase 2</b> and is currently under development.<br>
            Stay tuned for exciting AI-powered capabilities!</p>
        </div>""",
        unsafe_allow_html=True,
    )

    st.markdown("#### Planned Features")
    for feat in info["features"]:
        st.markdown(f"- {feat}")

    st.markdown("---")
    st.info("ğŸ“… **Phase 2 modules** are scheduled for future release. Contact the Jade Global team for early access.")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SETTINGS PAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def render_settings():
    render_page_header(
        "âš™ï¸",
        "Settings",
        "Configure your API credentials and application preferences.",
    )

    st.markdown("#### OpenAI API Configuration")
    st.markdown(
        "Provide your OpenAI API key to enable AI-powered modules. "
        "Keys are stored in session memory only and never persisted to disk."
    )

    new_key = st.text_input(
        "OpenAI API Key",
        value=st.session_state.get("openai_api_key", ""),
        type="password",
        placeholder="sk-...",
    )
    new_base = st.text_input(
        "OpenAI API Base URL (optional)",
        value=st.session_state.get("openai_api_base", ""),
        placeholder="https://api.openai.com/v1",
    )

    if st.button("Save Configuration", key="save_config"):
        st.session_state["openai_api_key"] = new_key
        st.session_state["openai_api_base"] = new_base
        if new_key:
            os.environ["OPENAI_API_KEY"] = new_key
        if new_base:
            os.environ["OPENAI_API_BASE"] = new_base
        st.success("Configuration saved successfully!")

    st.markdown("---")
    st.markdown("#### Application Information")
    info_col1, info_col2 = st.columns(2)
    with info_col1:
        st.markdown(f"**Application:** NexAiCare")
        st.markdown(f"**Version:** 1.0.0")
        st.markdown(f"**Developed by:** Jade Global")
    with info_col2:
        st.markdown(f"**AI Engine:** OpenAI GPT-4o-mini")
        st.markdown(f"**Vector Store:** ChromaDB")
        st.markdown(f"**Database:** SQLite")

    st.markdown("---")
    st.markdown("#### Module Status")
    modules = [
        ("Medical Assistant AI", "âœ… Active", "Phase 1"),
        ("Claim Audit AI", "âœ… Active", "Phase 1"),
        ("Consultation Notes AI", "âœ… Active", "Phase 1"),
        ("PII/PHI Monitor", "ğŸ”œ Coming Soon", "Phase 2"),
        ("Medical Coding AI", "ğŸ”œ Coming Soon", "Phase 2"),
        ("Clinical Diagnostic AI", "ğŸ”œ Coming Soon", "Phase 2"),
    ]
    status_df = pd.DataFrame(modules, columns=["Module", "Status", "Phase"])
    st.dataframe(status_df, use_container_width=True, hide_index=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SIDEBAR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def render_sidebar():
    with st.sidebar:
        # â”€â”€ Logo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logo_b64 = load_logo_b64(LOGO_DARK_PATH)
        if logo_b64:
            st.markdown(
                f"""<div class="logo-container">
                    <img src="data:image/png;base64,{logo_b64}" style="height:36px;">
                    <div>
                        <div class="logo-text">NexAiCare</div>
                        <div class="logo-subtitle">Healthcare AI Platform</div>
                    </div>
                </div>""",
                unsafe_allow_html=True,
            )
        else:
            st.markdown(
                """<div class="logo-container">
                    <div>
                        <div style="font-size:14px; font-weight:700; color:#FF6B35; letter-spacing:1px;">JADE</div>
                        <div style="font-size:9px; color:rgba(255,255,255,0.5); letter-spacing:2px;">GLOBAL</div>
                    </div>
                    <div>
                        <div class="logo-text">NexAiCare</div>
                        <div class="logo-subtitle">Healthcare AI Platform</div>
                    </div>
                </div>""",
                unsafe_allow_html=True,
            )

        st.markdown("---")

        # â”€â”€ Navigation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        selected = st.radio(
            "Navigation",
            options=list(MENU_ITEMS.keys()),
            index=list(MENU_ITEMS.keys()).index(st.session_state.selected_page),
            label_visibility="collapsed",
            key="nav_radio",
        )
        st.session_state.selected_page = selected

        # â”€â”€ API status indicator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("---")
        if api_key_configured():
            st.markdown("ğŸŸ¢ &nbsp; API Connected")
        else:
            st.markdown("ğŸ”´ &nbsp; API Key Required")

        # â”€â”€ Copyright â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        year = datetime.now().year
        st.markdown(
            f"""<div class="copyright">
                Â© {year} Jade Global. All rights reserved.<br>
                NexAiCare v1.0.0
            </div>""",
            unsafe_allow_html=True,
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MAIN APPLICATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    init_session_state()
    inject_css()
    render_sidebar()

    # Check if vectorstore already exists on disk (auto-detect)
    if os.path.exists(VECTOR_DB_DIR) and os.listdir(VECTOR_DB_DIR):
        st.session_state.vectorstore_ready = True

    page = MENU_ITEMS[st.session_state.selected_page]

    if page == "dashboard":
        render_dashboard()
    elif page == "module_1":
        render_module_1()
    elif page == "module_2":
        render_module_2()
    elif page == "module_3":
        render_module_3()
    elif page in ("module_4", "module_5", "module_6"):
        render_coming_soon(page)
    elif page == "settings":
        render_settings()


if __name__ == "__main__":
    main()
